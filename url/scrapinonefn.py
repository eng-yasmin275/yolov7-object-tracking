# -*- coding: utf-8 -*-
"""Scrapinonefn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rhS42rUErYCvshOynDwRmJGKflDBmob1

Follow the same steps as the "machine learning" jobs scraping
Add (Company Location)
Scrap all the pages of search results (not the first page only)
Repeat for : "data analysis" , "data science" , "business intelligence"
Collect all in ( one CSV File )
"""
import numpy as np 
import requests
from bs4 import BeautifulSoup
import pandas as pd

def Scrap(link):
  response = requests.get(link)
  soup = BeautifulSoup(response.content, 'lxml')
  titles = soup.find_all("h2", {'class':'css-m604qf'})
  titles_list = [title.text for title in titles]
  links_list = [link.a['href'] for link in titles]
  types = soup.find_all("a", {'class':'css-n2jc4m'})
  types_list = [type1.span.text for type1 in types]
  occupations = soup.find_all("div", {'class':'css-1lh32fc'})
  occupations_list = [occupation.text for occupation in occupations]
  companies = soup.find_all("a", {'class':'css-17s97q8'})
  companies_list = [company.text for company in companies]
  descrption = soup.find_all("div", {'class':'css-y4udm8'})
  descrption_list = [des.text for des in descrption]
  locations = soup.find_all("div", {'class': 'css-d7j1kk'})
  locations_list= [location.span.text for location in locations]


  sorted_dic={}
  sorted_dic['title'] = titles_list
  sorted_dic['link'] = links_list
  sorted_dic['type'] = types_list
  sorted_dic['occupation'] = occupations_list
  sorted_dic['company'] = companies_list
  sorted_dic['descrption'] = descrption_list
  sorted_dic['location'] = locations_list

  df = pd.DataFrame(sorted_dic)


  print('sussess')

  return df

import requests
from bs4 import BeautifulSoup
import pandas as pd

def ScrapAllPages(base_url, max_pages=None):
    all_data = []
    current_page = 1

    while True:
        # Construct the URL for the current page
        page_url = f"{base_url}&page={current_page}"  # Adjust query parameters as needed
        print(f"Scraping page: {current_page}")

        # Make the request
        response = requests.get(page_url)
        soup = BeautifulSoup(response.content, 'lxml')

        # Extract data from the page
        titles = soup.find_all("h2", {'class': 'css-m604qf'})
        titles_list = [title.text for title in titles]

        links_list = [link.a['href'] for link in titles]

        types = soup.find_all("a", {'class': 'css-n2jc4m'})
        types_list = [type1.span.text for type1 in types]

        occupations = soup.find_all("div", {'class': 'css-1lh32fc'})
        occupations_list = [occupation.text for occupation in occupations]

        companies = soup.find_all("a", {'class': 'css-17s97q8'})
        companies_list = [company.text for company in companies]

        descrption = soup.find_all("div", {'class': 'css-y4udm8'})
        descrption_list = [des.text for des in descrption]

        locations = soup.find_all("div", {'class': 'css-d7j1kk'})
        locations_list = [location.span.text for location in locations]

        # Combine data into a list of dictionaries
        page_data = [
            {
                'title': titles_list[i],
                'link': links_list[i],
                'type': types_list[i],
                'occupation': occupations_list[i],
                'company': companies_list[i],
                'descrption': descrption_list[i],
                'location': locations_list[i]
            }
            for i in range(len(titles_list))
        ]

        # Append the data for the current page
        all_data.extend(page_data)

        # Check if there's a next page or break the loop
        next_page = soup.find("a", {'rel': 'next'})
        if not next_page or (max_pages and current_page >= max_pages):
            break

        current_page += 1

    # Create a DataFrame from all collected data
    df = pd.DataFrame(all_data)
    print("Scraping completed.")
    return df

df = ScrapAllPages('https://wuzzuf.net/search/jobs/?q=machine%20learning&a=hpb', max_pages=15)
df

Machien_learning = Scrap('https://wuzzuf.net/search/jobs/?q=machine%20learning&a=hpb')
Machien_learning

data_analysis= Scrap('https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20analysis')

data_science = Scrap('https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20science')

business_intelligence = Scrap('https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=business%20intelligence')

import pandas as pd
result = pd.concat([Machien_learning,data_analysis, data_science,business_intelligence],ignore_index=True)
result.to_csv('jobs.csv', index=False)
